# Two proposals for the ingest-metadata KGX model - which will structure a file that complements the nodes and edges jsonlines files used to represent the graph itself.
# The first is a minimal model that covers only the high priority metadata that is accessible/feasibly in initial ingests, using a simple, flat model. 
# The second is a more structured, expressive model that defines general, re-usable data structures based on Biolink classes and modeling patterns/principles. This represents a possible longer term goal. 
# These proposals were informed by discussions on DINGO calls, and requirements described in the following documents:
    # https://docs.google.com/document/d/1cEDVPt2CejRDlFY7fMlZMBSly1be23XyH-FXSArDHaU/edit?tab=t.0
    # https://docs.google.com/presentation/d/12o4NFoLJO44wmL9bnvs2KB9z7JwN_tU6AqC66jgchgw/edit?slide=id.g35573e59b37_0_5#slide=id.g35573e59b37_0_5
    # https://docs.google.com/document/d/1_KbBahN8mArFvIfVqdhyzmEAGR4EXaHuK0HRbcVl4FA/edit?tab=t.0

# The model will be specified in linkML yaml, as a schema that is distinct from the Biolink Model, can be derived into a json schema, and may be imported into other linkML models as desired, inlcuding Biolink (similar to how the schema for the infores_catalog is specified/used)

# ----------------------------------------------------------------------------------------------- #

# PROPSOAL 1:  Minimal/flat Strawperson model. Slots included and their names/descriptions open for discussion. 

# Info about this metadata file
#"id":                          # identifier for this metadata file (may not need this) - OMIT
#"type":                        # biolink:IngestMetadata (subclassOf InformationContentEntity). Should we use a "category" slot here? - OMIT
 "title":                       # an informative, human-readable name for this metadata file/object, e.g. "CTD Chemical-Disease Source-KGX Ingest Metadata"   |   1..1 string
 "created_by":                  # who created this ingest metadata file    |   1..m string   (for now . . . may evolve to require identifiers (e.g. ORCIDs), or Agent instances/objects)
 "creation_date":               # when this metadata file was created                      |   1..1 date

# Metadata about the ingest task/process/code
#"ingest_contributors":         # list of developers/curators/modelers who contributed to creating the ingest - OMIT (creators/contributors can be determined from reviewing metadata/commit logs in GitHub code base - can move into ingest meta file if need arises)
#"ingest_contact_person":       # name of the point of contact about this ingest - OMIT for first pass
#"ingest_contact_email":        # email address to use to contact this person - OMIT for  first pass
 "ingest_code_url":             # url a specific official release or tagged branch of the code used to generate the target product     |    1..1 iriOrCurie     TO DO: explore hwo this would actually work
 "ingest_guide_url":            # url of the referecne ingest guide (RIG) that provides a free-text description of how and why data from a source was ingested, following defined RIG template/conventions
 
# General and ingest-specific info about the ingest source(s)
 "source_infores_id":           # infores identifier of the source                   |   1..1 iriOrCurie
 "source_license":              # license / terms of use for the ingested source.    |   0..1 string      TO DO: Provide additional guidance on how to populate. Consider a more cosntrained or richer model of licenses (see proposal 2 below)
 "source_data_version":         # version of the source data ingested (use version naming conventions/syntax from source in the value here)                     |      1..1 string
 "source_publication_date":     # date that the ingested data was actually published (or last updated) by the source (this may or may not be reflected in the source_data_version).    |      0..1 date
 "source_access_date":          # date that data from the resource was accessed and downloaded into the ingest system.   |    0..1 date
 "source_access_urls":          # urls or urls where source data was accessed / downloaded / queried        |    0..m iriOrCURIE
 "source_data_formats":         # format/type of files or payloads that were downloaded/retrieved.          |    0..m   string 
 "source_data_files":           # list of file names from which content was ingested (use when source data is in files)      |   0..m   string 
#"source_data_endpoints":       # list of API endpoint names/descriptions used to query/ingest source data (use when source data is accessed from an API) - OMIT, until we hit this use case
#"source_database":             # name/description of a database that was accessed/instantiated to retrieve source data (use when source data is pulled from a database) - OMIT, until we hit this ue case
 
# Metadata about the dataset/graph produced by the ingest (consider what applies to an intermediate KGX- artifact, vs a final KGX+, normalized, Biolink-compliant artifact). 
 "target_title":                # a unique human readable name of the target data/graph produced by this ingest.  e.g. "CTD Chemical-Disease Source-KG", or "CTD Chemical-Disease Biolink-KG".     |   1..1   string
 "target_type":                 # a flag that reports the levelof processing/modeling applied to the KG  {"Source-KG"  | "Biolink KG"  |  "Normalized KG" } . . .  TO DO: consider if we want something like this?    |   1..1   string / enum
#"target_id":                   # identifier of the target data/graph, as assigned by the ingesting system or ingest catalog (t.b.d. what this is, how these will look) - OMIT for now
 "target_data_version":         # version of the ingested data as produced by the ingesting system (Translator will use standard semantic versioning conventions here)      |   1..1   string
 "target_creation_date":        # date that the target data/graph (as serialized in the accompanying nodes and edges files) was created.    |    1..1 date
 "target_license":              # license / terms of use for the target data/graph. Consider a more cosntrained or richer model of licenses (see proposal 2 below)     |   1..1   string        TO DO: settle on license under which our data is released . . .or OMIT until we do?
 #"target_access_urls":         # url or urls where target data can be accessed / downloaded (should include page for it in our KG Registry/Hub, that gives more info about the dataset not provided in this metadata file) - OMIT until we set up infrastructure for this?
 "target_data_format":          # the format in which the data/graph is serialized. Will be "KGX" for all Translator KGs (KGX is a higher order format, which specifies a json-lines serialization)     |   1..1   string 
 "target_data_model":           # the model used to structure the graph data. Will be Biolink Model for all Translator KGs (except directly parsed "Source KGX" graphs). Reference using the url "https://w3id.org/biolink/biolink-model"?   |   0..1   string 
 "target_data_model_version":   # the version of this data model, e.g. "4.2.6-rc5     |   0..1   string
 "node_normalizer":             # the name of the node normalization tool/algorithm applied to the graph (for KGs that have been normalized). e.g. ""Babel"       |   0..1   string
 "node_normalizer_version":     # the version of the node normalization tool / algorithm        |   0..1   string


# Descriptive metadata about the content of the target (could be used for validation) - perhaps only for the final Normalized KGX artifact, not intermediate KGX artifacts?   OMIT all for now until we sort out waht to colelct,andwhere (may fit better in MetaKG?)
#"total_edge_count":           # Count of total number of edges in the graph
#"total_node_count":           # Count of total number of nodes in the graph
#"orphan_node_count":          # Count of nodes that do not participate in an edge 
#"node_categories":            # A list of biolink node categories used in the graph
#"edge_predicates":            # A list of biolink predicates used in the graph


# Other important info to include in the minimal version of this model / file?
  # validation metadata (how many edges failed normalization, or failed validation?)
  # a list of edge types (metaedges) represented in the ingest (reported as Scat - predicate - Ocat triples) . . . possible also with KL/AT for each?
  #  transform metadata (how data was transformed from source to target) - IMO this should be captured with / derived from code that executed the transformation, and documented/hosted elsewhere
  # . . . 


# ----------------------------------------------------------------------------------------------- #

# PROPSOAL 2:  Structured, expressive, flexible model - a longer term target that may serve as a broader Biolink-based standard.
# Proposes several classes including IngestMetadata, SourceIngest, Document, File, APIResult, DatabaseDump, DataSet, DataSetDistribution, MetaKnowlegeGraph, BiolinkTransformMetadata, and PropertyDerivation 

{

# Info about this metadata file (follows an 'IngestMetadata' schema)

 "id":                          # identifier for this metadata file (may not need this)
 "type":                        # biolink:IngestMetadata (subclassOf InformationContentEntity)
 "name":                        # a name for this metadata file, e.g. "ctd_chem_disease_ingest_meta",
 "created_by":                  # who created this ingest metadata file
 "creation_date":               # when it was created


# Metadata about the ingest process and code

 "ingest_team":                 # for us, this is the KP that performed the ingest, e.g. "infores:molepro" (could be their name, infores, or an Organization instance/object
 "ingest_contributors":         # list of developers/curators/modelers who contributed to creating the inges (free text names, person IDs (e.g ORCIDs), or Person objects
 "ingest_contact_person":       # name of the point of contact about this ingest  (free text, or Person object)
 "ingest_contact_email":        # email address to use to contact this person
 "ingest_code_url":             # url of link to code on Github
 "ingest_use_case_description": # free-text description of why data from a source was ingested, e.g. "Chemical-Disease Edges ingested to support . . . "
 

# Metadata about the ingested source(s)

 "ingest_sources": [            # one or more SourceIngest objects, describing the information resources, the data they provided, and how the data was retrieved. An array b/c some ingests use 'primary' and 'secondary' sources
   {
     "type":                    # biolink:SourceIngest
     
     "resource": {              # the Information Resource directly accessed to perform the ingest
       "type":                  # "biolink:InformationResource" - we cold build on the initial InfoRes schema described here: https://biolink.github.io/information-resource-registry/
       "id":                    # infores of the resource e.g. "infores:ctd"
       "publications":          # publications that my be helpful for understanding the source data ingested (list of ids, or biolink:Publication objects?)
       "documentation_urls":    # webpages that provide general info about the source, or describes downloads/services provided for accessing data (consdier if we want separate properties for these two categories of documentation, e.g. "general_documentation_urls", and "access_documentation_urls")
       "license": {             # a "Document" object that represents/describes the license that reports terms of use this resource.  Note that license may vary for different subsets/files provided by a source - which can be reported on individual files below
         "id":                  # identifier for the License document (if it has one)
         "type":                # "biolink:Document"
         "name":                # name of a public license, if one was used (e.g. cc-by)
         "url":                 # url of a publically accessible docuemnt representing the license or terms of use
         "description":         # free text description of terms of use
         }
     },

     "data_version":            # version of the source data ingested, as assigned by the source (we can capture this here if the same version applies across all content retrieved)
     "data_publication_date":   # date that the ingested data was actually published by the source (may or may not be reflected in the source_data_version)  "source_data_version":       
     "retrieval_date":          # date that data from the resource was retrieved and pulled into the target system (e.g. a file downloaded, a database dump generated, API queries run)
     "ingest_role":             # indicates if this is the 'primary' source, or a 'secondary' source contributing to this ingest  (some ingests rely on data from secondary sources to support the primary source  - e.g. DrugApprovalsKP uses FAERS as a secondary source to inform which/how edges are created).
     
    # Descriptions of the specific data artifacts retrieved. Below we define separate slots to capture info about the there primary types of mechanisms for retrieving data ('Files', 'API Endpoints', or 'Database Dumps') - but other approaches possible here.*
     "retrieved_files": [         # list of File objects, describing a file and when/how it was accessed
       {
         "type":                 # "biolink:File"
         "name":                 # name given to the file by the source, e.g. "CTD_chemicals_diseases.tsv.gz"
         "urls":                 # specific url for viewing/downloading the file
         "data_format":          # e.g. "tsv"
         "data_version":         # version of the specific file (if specific to this file, and not the larger ingested resource)
         "records":              # number of records in the source file (e.g. rows for a tsv file)
         "size":                 # e.g. "465 MB"
         # . . .                 # additional properties here? 
       }
     ],
     "retrieved_api_results": [  # list of API Result objects
       {
         "type":                 # "biolink:APIResult"
         "endpoint_name":        # name of a endpoint queried to return ingested data
         "endpoint_url":         # url of a endpoint queried to return ingested data
         "endpoint_version":     # version of the api queried
         "data_format":          # e.g. "json"
         "data_version":         # version of the specific file (if specific to this file, and not the larger ingested resource)
         "query_parameters":     # the query parameters used in the query
         # . . .                 # additional useful properties here? 
       }
     ],
     "retrieved_database_dump": [   # list of DatabaseDump objects
       {
         "type":                    # "biolink:DatabaseDump"
         "database_name":           # name of a database queried to return ingested data (if relevant/distinct form the name of the overall resource)
         "access_url":              # url where database accessed
         "data_format":             # e.g. "json"
         "data_version":            # version of the specific file (if specific to this file, and not the larger ingested resource)
         "query_parameters":        # the query parameters used in the query to generate the dump
         # . . .                    # additional useful properties here? 
       }       
     ],   
   },
 ]

# Metadata about the final biolink-ified dataset produced by the ingest. 
# Uses a 'Dataset' object to capture the info below, as the value of a single "targetDataset" property.
# A dataset may be a single file or a collection of many files whose data is consisdered part of the same data set (e.g. nodes and edges and ingest-metadata files in KGX+)

 "target_dataset": {
   "id":                 # a unique identifier for the graph/dataset produced by this ingest (this could be an infores, if we allow minting infores ids at level of each ingest, rather than a complete resource)
   "type":               # "biolink:DataSet"  (represents an HCLS "version" level artifact - see https://www.w3.org/TR/hcls-dataset/)
   "name":               # human readable name for the dataset/graph created by the ingest
   "version":            # version of the dataset (using conventions defined by creator)
   "description":        # free-text description of the dataset/graph (its contents, purpose, etc)
   "creation_date":      # data that this version of the dataset was created 
   "access_location":    # a url or file path where the content of this dataset version can be accessed
   "homepage":           # a url where more info about the dataset version and its contents, purpose, etc. can be found
   "license":            # use same model here as for the source license info captured above.
   "data_model":         # the data model used to structure the data, e.g. "infores:biolink-model"
   "data_model_version": # the version of this data model, e.g. "4.2.6-rc5"
   "format":             # a format for the dataset as a whole (may be differnet than the format used for each file).  e.g. here, "KGX" is a higher order format/standard than the concrete serialization formats of each file below
   "distributions": [    # A list of 'DataSetDistribution' objects, representing represents an HCLS 'distribution' level artifacts. Groups proeprties of the DataSet that hold at the 'distribution' level.
     {
      "type":            # "biolnk:DataSetDistribution" (represents an HCLS "distribution" level artifact - see https://www.w3.org/TR/hcls-dataset/)
      "name":            # descriptive, human-readable name for the distribution
      "files":           # list of files that are part of this DataSet version, e.g. ["ctd_chem_disease_nodes.jsonl", "ctd_chem_disease_edges.jsonl", "ctd_chem_disease_ingest_meta.yaml"]  
      "access_urls":     # pages where files can be accessed
      "endpoints":       # list of endpoints/services through which data from this distribution can be accessed programmatically
      "formats":         # formats used for data in this distribution (e.g. "jsonl, yaml")
      }	
    ],

   # Alternatively, we could use a more expressive structure that allows description of each file in a Disctribution.  Just replace "distributions" property above with:
   "files": [            # a list of files in the target dataset, represented as 'File' objects, which may include descriptive info unique to each file and its contents - but this info might be provided / providable in the MetaKnowledgeGraph structure further below. Note also that this info could instead go in the header of the files themselves. 
     {
       "type":           # "biolink:File"
       "name":           # e.g. "ctd_chem_disease_nodes.jsonl",
       "format":         # e.g. "json-lines",
       "records":        # number of records (nodes) in the file
       "size":           # e.g. "655 MB"
       "urls":           # specific url for viewing/downloading the file
       # . . .           # additional useful properties here? 
     },
     {
       "type":           # "biolink:File"
       "name":           # e.g. "ctd_chem_disease_edges.jsonl",
       "format":         # e.g. "json-lines",
       "records":        # number of records (edges) in the file
       "size":           # e.g. "1035 MB"
       "urls":           # specific url for viewing/downloading the file
       # . . .           # additional useful properties here? 
     },
     {
       "type":           # "biolink:File"
       "name":           # e.g. "ctd_chem_disease_ingest_meta.jsonl",
       "format":         # e.g. "yaml",
       "records":        # number of records in the file  (there will only be one ingest record per ingest metadata file)
       "size":           # e.g. "5 MB"
       "urls":           # specific url for viewing/downloading the file
       # . . .             # additional useful properties here? 
   },
 ],
 
 
 # Metadata about the types of nodes, edges and attributes in the target dataset/graph. 
 "target_metakg": [
   {
     # Leverage/refine the TRAPI MetaKnowledgeGraph schema here. Consider enhancing the schema to capture all known knowledge sources for each MetaEdge in a MetaKG  - e.g. 'primary', 'aggregator' (if ingested from an external aggregator like Pharos), 'supporting_data' (for KPs that create de novo knowledge by interpreting/analyzing more foundational data). Would just require addition of one property def to the MetaEdge schema ("sources": { "type": "array", "items": $ref: "#/components/schemas/RetrievalSource" } )
   },


# Metadata about the source-to-target derivation.
# Note that these 'derivations' are not meant to be complete declarative mappings or transform specifications. They are meant only to provide an idea of what fields in a source dataset/file provided information used to populate what edge properties in the Biolink-compliant representation - so that a developer can get a sense of where to look to evaluate ingest accuracy/quality, debug, or re-engineer/refactor the ingest, etc. 

 "transform_metadata": 
   {
   "type": "biolink:BiolinkTransformMetadata"
   "spoq_property_derivations": [
     {
       "type": "biolink:PropertyDerivation"
       "source_properties": ["CTD_chemicals_diseases.tsv.gz:ChemicalID"],   # field name or path to element holding the data used to create the biolink.subject value. Prefix with name of source file to disambiguate if needed. Multivalued as some biolink properties hold content derived form >1 source field. 
       "target_property": "biolink:subject",
       "comments":               # any relevant/useful info the developer wants to provide about this derivation (e.g pre-processing/filtering steps or logic)
     },	
     {
       "type": "biolink:PropertyDerivation"
       "source_properties": ["CTD_chemicals_diseases.tsv.gzDiseaseID"],
       "target_property": "biolink:object",
       "comments": 
     },
     {
       "type": "biolink:PropertyDerivation"
       "source_properties": ["CTD_chemicals_diseases.tsv.gzDirectEvidence"],
       "target_property": "biolink:predicate",
       "comments": 
     } 
   ],

   "edge_property_derivations": [
     {
       "type": "biolink:PropertyDerivation"
       "source_properties": ["CTD_chemicals_diseases.tsv.gz:PubMedIDs"],
       "target_property": "biolink:publications",
       "comments": 
     },	
     {
       "type": "biolink:PropertyDerivation"
       "source_properties": ["CTD_chemicals_diseases.tsv.gz:InferenceScore"],
       "target_property": "biolink:score",
       "comments": 	 
     }
   ],
 

# Metadata about the intermediate pre-normalized/biolink-ified dataset produced by a direct, unopinionated parsing of the source data described above. 
# Uses a 'Dataset' object to capture the info below, as the value of a single "intermediateDataset" property.

 "intermediate_dataset":  {
   # Same data model as used for describing the final target dataset above
 },










# ---------------------------------------------------------------------------------------------------- #


# Footnotes:

# *   Alt, we could have a single "ingested_data_artifacts" slot that takes a File, API Endpoint, or Database object). Or an "ingested_dataset" slot that takes a DataSet representing just what we ingested, 
# and which can be comprised of one or more files. Note that I don't suggest creating a DataSet object for the source data (as I did for the target KGX output), as here we are interested in
# some ad hoc set of files or API query results, which may not be an actual 'dataset' provided by the source. I don't see value in creating a DataSet object in this case. 
